{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4503bbef-9009-4ca0-a123-715713173a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from read_data import ChestXrayDataSet\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d8f456-7ce3-49de-a728-672cc5ae4eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = 'model.pth.tar'\n",
    "N_CLASSES = 14\n",
    "CLASS_NAMES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
    "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
    "DATA_DIR = './ChestX-ray14/images'\n",
    "TEST_IMAGE_LIST = './ChestX-ray14/labels/test_list.txt'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04b54c9c-b30a-4639-84e3-3e384ca3874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.7.1+cu118\n",
      "cuda available: True\n",
      "cuda runtime: 11.8\n",
      "allocated (bytes): 65870336\n",
      "reserved  (bytes): 75497472\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  64326 KiB | 397692 KiB |   2871 GiB |   2871 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  64326 KiB | 397692 KiB |   2871 GiB |   2871 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  64114 KiB | 395944 KiB |   2865 GiB |   2865 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  73728 KiB | 544768 KiB | 544768 KiB | 471040 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   9401 KiB | 188804 KiB |   3624 GiB |   3624 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1463    |    1491    |  262655    |  261192    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1463    |    1491    |  262655    |  261192    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      44    |      44    |      17    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       5    |      19    |  121414    |  121409    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda runtime:\", torch.version.cuda)\n",
    "print(\"allocated (bytes):\", torch.cuda.memory_allocated())\n",
    "print(\"reserved  (bytes):\", torch.cuda.memory_reserved())\n",
    "print(torch.cuda.memory_summary(abbreviated=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c7e01d9-4b7f-4aa0-a111-aeb35487ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # initialize and load the model\n",
    "    # model = DenseNet121(N_CLASSES).cuda()\n",
    "    # model = torch.nn.DataParallel(model).cuda()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = DenseNet121(N_CLASSES).to(device)\n",
    "    # use DataParallel only if you actually have >1 GPU\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    "    if os.path.isfile(CKPT_PATH):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        checkpoint = torch.load(CKPT_PATH)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        print(\"=> loaded checkpoint\")\n",
    "    else:\n",
    "        print(\"=> no checkpoint found\")\n",
    "\n",
    "    normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])\n",
    "\n",
    "    test_dataset = ChestXrayDataSet(data_dir=DATA_DIR,\n",
    "                                    image_list_file=TEST_IMAGE_LIST,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.Resize(256),\n",
    "                                        transforms.TenCrop(224),\n",
    "                                        transforms.Lambda\n",
    "                                        (lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                                        transforms.Lambda\n",
    "                                        (lambda crops: torch.stack([normalize(crop) for crop in crops]))\n",
    "                                    ]))\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # initialize the ground truth and output tensor\n",
    "    # gt = torch.FloatTensor()\n",
    "    # gt = gt.cuda()\n",
    "    # pred = torch.FloatTensor()\n",
    "    # pred = pred.cuda()\n",
    "    gt = torch.empty((0, N_CLASSES), dtype=torch.float32, device=device)\n",
    "    pred = torch.empty((0, N_CLASSES), dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inp, target) in enumerate(test_loader):\n",
    "        # target = target.cuda()\n",
    "        # gt = torch.cat((gt, target), 0)\n",
    "        # bs, n_crops, c, h, w = inp.size()\n",
    "        # input_var = torch.autograd.Variable(inp.view(-1, c, h, w).cuda(), volatile=True)\n",
    "        # output = model(input_var)\n",
    "        # output_mean = output.view(bs, n_crops, -1).mean(1)\n",
    "        # pred = torch.cat((pred, output_mean.data), 0)\n",
    "\n",
    "        bs, n_crops, c, h, w = inp.size()\n",
    "\n",
    "        # move inputs to device (once) and reshape for TenCrop\n",
    "        inp = inp.to(device)                          # shape (bs, n_crops, c, h, w)\n",
    "        input_tensor = inp.view(-1, c, h, w)          # shape (bs * n_crops, c, h, w)\n",
    "        \n",
    "        # inference with no grad + mixed precision (saves memory)\n",
    "        with torch.no_grad():\n",
    "            from torch.cuda.amp import autocast\n",
    "            with autocast():\n",
    "                output = model(input_tensor)          # (bs * n_crops, n_classes)\n",
    "                output_mean = output.view(bs, n_crops, -1).mean(1)   # (bs, n_classes)\n",
    "        \n",
    "        # accumulate predictions to pred (keep pred on GPU or CPU as you prefer)\n",
    "\n",
    "        # change in code here\n",
    "        target = target.to(device)\n",
    "        gt = torch.cat((gt, target), 0)\n",
    "        \n",
    "        pred = torch.cat((pred, output_mean.detach().to(pred.device)), 0)\n",
    "        \n",
    "        # --- immediate cleanup of large temporaries to free GPU RAM ---\n",
    "        del input_tensor, output, output_mean, inp\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            torch.cuda.ipc_collect()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # # --- safe GPU cleanup (drop-in) ---\n",
    "        # try:\n",
    "        #     # delete large per-batch tensors if they exist\n",
    "        #     del inp\n",
    "        # except NameError:\n",
    "        #     pass\n",
    "        \n",
    "        # for _name in (\"input_var\", \"output\", \"output_mean\", \"target\", \"images\", \"labels\"):\n",
    "        #     try:\n",
    "        #         del globals()[_name]\n",
    "        #     except Exception:\n",
    "        #         # NameError / KeyError / others — ignore if not present\n",
    "        #         pass\n",
    "        \n",
    "        # # run python garbage collector\n",
    "        # gc.collect()\n",
    "        \n",
    "        # # free CUDA cached memory (makes memory available to other processes)\n",
    "        # torch.cuda.empty_cache()\n",
    "        \n",
    "        # # optional: collect shared CUDA IPC (safe to call)\n",
    "        # try:\n",
    "        #     torch.cuda.ipc_collect()\n",
    "        # except Exception:\n",
    "        #     pass\n",
    "        \n",
    "        # # optional: brief memory summary (prints CUDA info if available)\n",
    "        # try:\n",
    "        #     print(torch.cuda.memory_summary(device=None, abbreviated=True))\n",
    "        # except Exception:\n",
    "        #     # fallback: print basic allocated/reserved info\n",
    "        #     if torch.cuda.is_available():\n",
    "        #         print(f\"allocated: {torch.cuda.memory_allocated()} bytes, \"\n",
    "        #               f\"reserved: {torch.cuda.memory_reserved()} bytes\")\n",
    "        #     else:\n",
    "        #         print(\"CUDA not available\")\n",
    "        # # --- end cleanup ---\n",
    "\n",
    "\n",
    "    AUROCs = compute_AUCs(gt, pred)\n",
    "    AUROC_avg = np.array(AUROCs).mean()\n",
    "    print('The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "    for i in range(N_CLASSES):\n",
    "        print('The AUROC of {} is {}'.format(CLASS_NAMES[i], AUROCs[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5389a9e-0afb-4646-a4a4-b6bef35ff409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AUCs(gt, pred):\n",
    "    \"\"\"Computes Area Under the Curve (AUC) from prediction scores.\n",
    "\n",
    "    Args:\n",
    "        gt: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          true binary labels.\n",
    "        pred: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          can either be probability estimates of the positive class,\n",
    "          confidence values, or binary decisions.\n",
    "\n",
    "    Returns:\n",
    "        List of AUROCs of all classes.\n",
    "    \"\"\"\n",
    "    AUROCs = []\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    for i in range(N_CLASSES):\n",
    "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "    return AUROCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7bb67a8-7a80-477f-85bf-763d47aaedf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 2.7.1+cu118\n",
      "cuda available: True\n",
      "cuda device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch.__version__:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817be0cd-9805-4686-b2d1-f6d51a9e8d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "CUDA runtime version: 11.8\n",
      "Device count: 1\n",
      "Device name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA runtime version:\", torch.version.cuda)\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4272975b-3a76-4eff-82d0-6288726240e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "train_list.txt and val_list.txt already present.\n",
      "Train/Val sizes: 78468 11219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/5 done, loss=0.7472\n",
      "Batch 2/5 done, loss=0.5521\n",
      "Batch 3/5 done, loss=0.4363\n",
      "Batch 4/5 done, loss=0.3429\n",
      "Batch 5/5 done, loss=0.2618\n",
      "Quick train done on 6 batches, avg loss: 0.3900510420401891\n",
      "Validation mean AUROC (smoke-test): 0.5091742308442769\n",
      "Per-class AUROC (first 6): [0.5818910094762828, 0.5310147023104714, 0.564197550789494, 0.5117441020167477, 0.4609779120256749, 0.4655738279818835]\n",
      "Saved checkpoint to model.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# ---- REPLACE current DenseNet121 class with this ----\n",
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"DenseNet121 backbone with a linear multi-label head (no Sigmoid).\"\"\"\n",
    "    \n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        # Using pretrained weights; newer versions may warn about 'pretrained', but this works\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        \n",
    "        # Get the number of input features to the classifier\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        \n",
    "        # Replace the classifier with a linear layer that outputs logits\n",
    "        # (No sigmoid activation here — handle it in the loss function if needed)\n",
    "        self.densenet121.classifier = nn.Linear(num_ftrs, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return x\n",
    "# -----------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------- TRAINING SMOKE-TEST CELL (FAST: runs 5 batches) ----------\n",
    "import os, random, numpy as np, torch\n",
    "import torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torchvision\n",
    "\n",
    "# ---------- 1) basic env + reproducibility ----------\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------- 2) ensure train/val lists exist (split test_list.txt if needed) ----------\n",
    "labels_dir = os.path.join(\".\", \"ChestX-ray14\", \"labels\")\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "master_list = os.path.join(labels_dir, \"test_list.txt\")\n",
    "train_list = os.path.join(labels_dir, \"train_list.txt\")\n",
    "val_list = os.path.join(labels_dir, \"val_list.txt\")\n",
    "\n",
    "if not os.path.exists(train_list) or not os.path.exists(val_list):\n",
    "    if os.path.exists(master_list):\n",
    "        with open(master_list, 'r') as f:\n",
    "            lines = [l.strip() for l in f if l.strip()]\n",
    "        random.shuffle(lines)\n",
    "        n = len(lines)\n",
    "        n_train = int(0.8*n); n_val = int(0.1*n)\n",
    "        train_lines = lines[:n_train]\n",
    "        val_lines = lines[n_train:n_train+n_val]\n",
    "        test_lines = lines[n_train+n_val:]\n",
    "        with open(train_list, 'w') as f: f.write(\"\\n\".join(train_lines))\n",
    "        with open(val_list, 'w') as f: f.write(\"\\n\".join(val_lines))\n",
    "        with open(os.path.join(labels_dir, 'test_list_split.txt'), 'w') as f: f.write(\"\\n\".join(test_lines))\n",
    "        print(\"Created train/val/test split:\", len(train_lines), len(val_lines), len(test_lines))\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No master list found at {master_list}. Place a list or tell me the correct path.\")\n",
    "else:\n",
    "    print(\"train_list.txt and val_list.txt already present.\")\n",
    "\n",
    "# ---------- 3) transforms ----------\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "val_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ---------- 4) dataset & dataloader (uses your read_data.ChestXrayDataSet) ----------\n",
    "try:\n",
    "    from read_data import ChestXrayDataSet\n",
    "except Exception as e:\n",
    "    print(\"ERROR importing ChestXrayDataSet from read_data:\", e)\n",
    "    raise\n",
    "\n",
    "DATA_DIR = globals().get('DATA_DIR', os.path.join(\".\", \"ChestX-ray14\", \"images\"))\n",
    "N_CLASSES = globals().get('N_CLASSES', 14)\n",
    "CKPT_PATH = globals().get('CKPT_PATH', os.path.join(\".\", \"model.pth.tar\"))\n",
    "\n",
    "train_dataset = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=train_list, transform=train_transform)\n",
    "val_dataset   = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=val_list, transform=val_transform)\n",
    "\n",
    "# Use multiple workers and pin_memory to speed up data loading (good with GPU)\n",
    "BATCH_SIZE = 16   # reduce if OOM\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "print(\"Train/Val sizes:\", len(train_dataset), len(val_dataset))\n",
    "\n",
    "# ---------- 5) model (assumes DenseNet121 class already redefined without Sigmoid) ----------\n",
    "model = DenseNet121(N_CLASSES).to(device)\n",
    "\n",
    "# freeze everything except classifier head\n",
    "for name, p in model.named_parameters():\n",
    "    if \"densenet121.classifier\" not in name:\n",
    "        p.requires_grad = False\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "# ---------- 6) QUICK train: only first few batches (smoke test) ----------\n",
    "model.train()\n",
    "running_loss = 0.0\n",
    "max_batches = 5\n",
    "for i, (imgs, targets) in enumerate(train_loader):\n",
    "    if i >= max_batches:\n",
    "        break\n",
    "    imgs = imgs.to(device); targets = targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(imgs)             # logits\n",
    "    loss = criterion(logits, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item() * imgs.size(0)\n",
    "    print(f\"Batch {i+1}/{max_batches} done, loss={loss.item():.4f}\")\n",
    "\n",
    "if running_loss == 0.0:\n",
    "    print(\"Warning: no batches were processed (train_loader may be empty).\")\n",
    "else:\n",
    "    print(\"Quick train done on\", i+1, \"batches, avg loss:\", running_loss / ((i+1) * imgs.size(0)))\n",
    "\n",
    "# ---------- 7) quick validation ----------\n",
    "model.eval()\n",
    "all_logits, all_gt = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, targets in val_loader:\n",
    "        imgs = imgs.to(device); targets = targets.to(device)\n",
    "        logits = model(imgs)\n",
    "        all_logits.append(logits.cpu()); all_gt.append(targets.cpu())\n",
    "\n",
    "if len(all_logits) == 0:\n",
    "    print(\"Warning: validation loader produced 0 batches.\")\n",
    "    all_logits = torch.empty((0, N_CLASSES))\n",
    "    all_gt = torch.empty((0, N_CLASSES))\n",
    "else:\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_gt = torch.cat(all_gt, dim=0)\n",
    "\n",
    "probs = torch.sigmoid(all_logits).numpy() if all_logits.numel() > 0 else np.empty((0, N_CLASSES))\n",
    "gt_np = all_gt.numpy() if all_gt.numel() > 0 else np.empty((0, N_CLASSES))\n",
    "\n",
    "aucs = []\n",
    "for i in range(N_CLASSES):\n",
    "    if gt_np.shape[0] == 0 or len(np.unique(gt_np[:,i])) < 2:\n",
    "        aucs.append(float('nan'))\n",
    "    else:\n",
    "        try:\n",
    "            aucs.append(float(roc_auc_score(gt_np[:,i], probs[:,i])))\n",
    "        except Exception:\n",
    "            aucs.append(float('nan'))\n",
    "\n",
    "mean_auc = np.nanmean(aucs) if len(aucs) > 0 else float('nan')\n",
    "print(\"Validation mean AUROC (smoke-test):\", mean_auc)\n",
    "print(\"Per-class AUROC (first 6):\", aucs[:6])\n",
    "\n",
    "# ---------- 8) save checkpoint ----------\n",
    "state = {'epoch': 1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'val_mean_auc': float(mean_auc)}\n",
    "torch.save(state, CKPT_PATH)\n",
    "print(\"Saved checkpoint to\", CKPT_PATH)\n",
    "# ---------- END ----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e4af3c7-764f-479d-9a13-16c42667834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists: True\n",
      "keys: ['epoch', 'state_dict', 'optimizer', 'val_mean_auc']\n",
      "epoch: 1 val_mean_auc: 0.5091742308442769\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "print(\"exists:\", os.path.exists(\"model.pth.tar\"))\n",
    "if os.path.exists(\"model.pth.tar\"):\n",
    "    ckpt = torch.load(\"model.pth.tar\", map_location='cpu')\n",
    "    print(\"keys:\", list(ckpt.keys()))\n",
    "    print(\"epoch:\", ckpt.get('epoch'), \"val_mean_auc:\", ckpt.get('val_mean_auc'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a1ea960-3b4e-4c4e-aadd-0dff90c020da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels dir exists: True\n",
      "Files in labels dir: ['test_list.txt', 'test_list_split.txt', 'train_list.txt', 'val_list.txt']\n",
      "Test list already exists at: .\\ChestX-ray14\\labels\\test_list_split.txt\n",
      "Final files in labels dir (sample): ['test_list.txt', 'test_list_split.txt', 'train_list.txt', 'val_list.txt']\n",
      "Test list path: .\\ChestX-ray14\\labels\\test_list_split.txt | exists: True\n",
      "Number of lines in test_list_split.txt: 22433\n",
      "First 10 lines (preview):\n",
      "  00011997_000.png 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  00011997_001.png 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  00011997_002.png 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  00011997_003.png 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  00011997_004.png 0 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "  00011997_005.png 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  00011997_006.png 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  00011997_007.png 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  00011997_008.png 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  00011997_009.png 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "###WE RUN THIS TO FIX THE TEST_LIST.TXT FILE ERROR IS FIXED ,I.E.-\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run this cell to create or locate test_list_split.txt automatically\n",
    "import os, random, shutil, glob\n",
    "\n",
    "labels_dir = os.path.join(\".\", \"ChestX-ray14\", \"labels\")\n",
    "images_dir = os.path.join(\".\", \"ChestX-ray14\", \"images\")\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "wanted = os.path.join(labels_dir, \"test_list_split.txt\")\n",
    "\n",
    "print(\"Labels dir exists:\", os.path.exists(labels_dir))\n",
    "print(\"Files in labels dir:\", os.listdir(labels_dir)[:200])\n",
    "\n",
    "if os.path.exists(wanted):\n",
    "    print(\"Test list already exists at:\", wanted)\n",
    "else:\n",
    "    # 1) prefer existing explicit test files\n",
    "    candidates = [\n",
    "        os.path.join(labels_dir, \"test_list.txt\"),\n",
    "        os.path.join(labels_dir, \"test.txt\"),\n",
    "        os.path.join(labels_dir, \"test_list_full.txt\")\n",
    "    ]\n",
    "    found = None\n",
    "    for c in candidates:\n",
    "        if os.path.exists(c):\n",
    "            found = c\n",
    "            break\n",
    "    # 2) more general search for any test*.txt\n",
    "    if not found:\n",
    "        for f in os.listdir(labels_dir):\n",
    "            if f.lower().startswith(\"test\") and f.lower().endswith(\".txt\"):\n",
    "                found = os.path.join(labels_dir, f)\n",
    "                break\n",
    "\n",
    "    if found:\n",
    "        shutil.copy(found, wanted)\n",
    "        print(f\"Copied existing test list {found} -> {wanted}\")\n",
    "    else:\n",
    "        # 3) if train_list exists, sample 10% into test_list_split (without modifying train_list)\n",
    "        train_f = os.path.join(labels_dir, \"train_list.txt\")\n",
    "        if os.path.exists(train_f):\n",
    "            with open(train_f, \"r\") as fh:\n",
    "                train_lines = [l.strip() for l in fh if l.strip()]\n",
    "            if len(train_lines) < 2:\n",
    "                print(\"train_list.txt exists but has too few lines to sample. Skipping.\")\n",
    "            else:\n",
    "                random.seed(42)\n",
    "                random.shuffle(train_lines)\n",
    "                n = len(train_lines)\n",
    "                n_test = max(1, int(0.1 * n))\n",
    "                test_lines = train_lines[:n_test]\n",
    "                with open(wanted, \"w\") as fh:\n",
    "                    fh.write(\"\\n\".join(test_lines))\n",
    "                print(f\"Created {wanted} by sampling {n_test} entries (10%) from train_list.txt (train_list NOT modified).\")\n",
    "        else:\n",
    "            # 4) fallback: create small test_list_split from image files\n",
    "            if os.path.exists(images_dir):\n",
    "                im_files = []\n",
    "                for ext in (\"*.png\",\"*.jpg\",\"*.jpeg\",\"*.PNG\",\"*.JPG\",\"*.JPEG\"):\n",
    "                    im_files.extend(glob.glob(os.path.join(images_dir, \"**\", ext), recursive=True))\n",
    "                if len(im_files) == 0:\n",
    "                    raise FileNotFoundError(\"No image files found in ChestX-ray14/images to create a test list.\")\n",
    "                random.seed(42)\n",
    "                random.shuffle(im_files)\n",
    "                n_test = min(100, max(1, int(0.01 * len(im_files))))  # 1% up to 100\n",
    "                chosen = im_files[:n_test]\n",
    "                # write relative paths with respect to images_dir (common format)\n",
    "                rel_paths = [os.path.relpath(p, images_dir).replace(\"\\\\\", \"/\") for p in chosen]\n",
    "                with open(wanted, \"w\") as fh:\n",
    "                    fh.write(\"\\n\".join(rel_paths))\n",
    "                print(f\"No label lists found. Created small {wanted} with {len(rel_paths)} image paths (relative to {images_dir}).\")\n",
    "            else:\n",
    "                raise FileNotFoundError(\"No labels and no images folder found. Please provide a test_list file.\")\n",
    "\n",
    "# Final confirmation\n",
    "print(\"Final files in labels dir (sample):\", os.listdir(labels_dir)[:200])\n",
    "print(\"Test list path:\", wanted, \"| exists:\", os.path.exists(wanted))\n",
    "if os.path.exists(wanted):\n",
    "    with open(wanted, \"r\") as fh:\n",
    "        lines = [l.strip() for l in fh if l.strip()]\n",
    "    print(\"Number of lines in test_list_split.txt:\", len(lines))\n",
    "    print(\"First 10 lines (preview):\")\n",
    "    for ln in lines[:10]:\n",
    "        print(\" \", ln)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c144679-8b42-4b65-ae20-65a912357143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: model.pth.tar\n",
      "Using test list: .\\ChestX-ray14\\labels\\test_list_split.txt\n",
      "Test dataset size: 22433\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Predictions shape: (22433, 14) GT shape: (22433, 14)\n",
      "\n",
      "Per-class AUROC:\n",
      " Class 00: AUROC = 0.5649   95% CI = [0.5530, 0.5776]\n",
      " Class 01: AUROC = 0.4903   95% CI = [0.4670, 0.5131]\n",
      " Class 02: AUROC = 0.5420   95% CI = [0.5307, 0.5539]\n",
      " Class 03: AUROC = 0.5305   95% CI = [0.5209, 0.5404]\n",
      " Class 04: AUROC = 0.4778   95% CI = [0.4626, 0.4966]\n",
      " Class 05: AUROC = 0.4536   95% CI = [0.4391, 0.4689]\n",
      " Class 06: AUROC = 0.5235   95% CI = [0.4888, 0.5589]\n",
      " Class 07: AUROC = 0.4881   95% CI = [0.4712, 0.5055]\n",
      " Class 08: AUROC = 0.4366   95% CI = [0.4189, 0.4548]\n",
      " Class 09: AUROC = 0.6378   95% CI = [0.6108, 0.6645]\n",
      " Class 10: AUROC = 0.4849   95% CI = [0.4598, 0.5118]\n",
      " Class 11: AUROC = 0.4339   95% CI = [0.4045, 0.4634]\n",
      " Class 12: AUROC = 0.4778   95% CI = [0.4578, 0.4995]\n",
      " Class 13: AUROC = 0.3112   95% CI = [0.2427, 0.3827]\n",
      "\n",
      "Mean AUROC: 0.4895\n",
      "Saved results to ./eval_results\n"
     ]
    }
   ],
   "source": [
    "# Cell B: Full evaluation main() - paste & run\n",
    "import os, torch, torchvision, numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# === User config (edit if needed) ===\n",
    "DATA_DIR = globals().get('DATA_DIR', os.path.join(\".\", \"ChestX-ray14\", \"images\"))\n",
    "TEST_IMAGE_LIST = os.path.join(\".\", \"ChestX-ray14\", \"labels\", \"test_list_split.txt\")  # adjust if different\n",
    "CKPT_PATH = globals().get('CKPT_PATH', os.path.join(\".\", \"model.pth.tar\"))\n",
    "N_CLASSES = globals().get('N_CLASSES', 14)\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "\n",
    "# === helper: bootstrap CI for AUROC ===\n",
    "def auc_bootstrap(y_true, y_score, n_boot=1000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    scores = []\n",
    "    n = len(y_score)\n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.randint(0, n, n)\n",
    "        if len(np.unique(y_true[idx])) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            s = roc_auc_score(y_true[idx], y_score[idx])\n",
    "            scores.append(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if len(scores) == 0:\n",
    "        return (float('nan'), float('nan'))\n",
    "    scores = np.sort(scores)\n",
    "    low = scores[int(0.025 * len(scores))]\n",
    "    high = scores[int(0.975 * len(scores))]\n",
    "    return float(low), float(high)\n",
    "\n",
    "# === verify files ===\n",
    "print(\"Using checkpoint:\", CKPT_PATH)\n",
    "print(\"Using test list:\", TEST_IMAGE_LIST)\n",
    "if not os.path.exists(CKPT_PATH):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {CKPT_PATH}\")\n",
    "if not os.path.exists(TEST_IMAGE_LIST):\n",
    "    raise FileNotFoundError(f\"Test list not found at {TEST_IMAGE_LIST}\")\n",
    "\n",
    "# === dataset & dataloader ===\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "try:\n",
    "    from read_data import ChestXrayDataSet\n",
    "except Exception as e:\n",
    "    raise ImportError(f\"Failed to import ChestXrayDataSet from read_data: {e}\")\n",
    "\n",
    "test_dataset = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=TEST_IMAGE_LIST, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "print(\"Test dataset size:\", len(test_dataset))\n",
    "\n",
    "# === model: reuse DenseNet121 if defined, otherwise define minimal one ===\n",
    "try:\n",
    "    DenseNet121\n",
    "except NameError:\n",
    "    import torch.nn as nn\n",
    "    class DenseNet121(nn.Module):\n",
    "        def __init__(self, out_size):\n",
    "            super().__init__()\n",
    "            self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "            num_ftrs = self.densenet121.classifier.in_features\n",
    "            self.densenet121.classifier = nn.Linear(num_ftrs, out_size)\n",
    "        def forward(self, x):\n",
    "            return self.densenet121(x)\n",
    "\n",
    "# === load checkpoint robustly ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "# extract state_dict\n",
    "if 'state_dict' in ckpt:\n",
    "    sd = ckpt['state_dict']\n",
    "elif 'model_state_dict' in ckpt:\n",
    "    sd = ckpt['model_state_dict']\n",
    "else:\n",
    "    sd = ckpt\n",
    "# strip 'module.' prefix if needed\n",
    "new_sd = OrderedDict()\n",
    "for k,v in sd.items():\n",
    "    nk = k[len('module.'):] if k.startswith('module.') else k\n",
    "    new_sd[nk] = v\n",
    "\n",
    "model = DenseNet121(N_CLASSES).to(device)\n",
    "model.load_state_dict(new_sd, strict=False)\n",
    "model.eval()\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# === inference ===\n",
    "all_probs = []\n",
    "all_gt = []\n",
    "with torch.no_grad():\n",
    "    for imgs, targets in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)                   # logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        # convert targets safely\n",
    "        if isinstance(targets, torch.Tensor):\n",
    "            all_gt.append(targets.cpu().numpy())\n",
    "        else:\n",
    "            all_gt.append(np.asarray(targets))\n",
    "\n",
    "all_probs = np.vstack(all_probs)\n",
    "all_gt = np.vstack(all_gt)\n",
    "print(\"Predictions shape:\", all_probs.shape, \"GT shape:\", all_gt.shape)\n",
    "if all_probs.shape[0] == 0:\n",
    "    raise RuntimeError(\"No predictions produced (empty loader?)\")\n",
    "\n",
    "# === metrics: per-class AUROC + bootstrap CI ===\n",
    "per_class_auc = []\n",
    "per_class_ci = []\n",
    "for i in range(N_CLASSES):\n",
    "    y_true = all_gt[:, i]\n",
    "    y_score = all_probs[:, i]\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        per_class_auc.append(float('nan'))\n",
    "        per_class_ci.append((float('nan'), float('nan')))\n",
    "    else:\n",
    "        auc = float(roc_auc_score(y_true, y_score))\n",
    "        per_class_auc.append(auc)\n",
    "        low, high = auc_bootstrap(y_true, y_score, n_boot=1000)\n",
    "        per_class_ci.append((low, high))\n",
    "\n",
    "mean_auc = np.nanmean(per_class_auc)\n",
    "print(\"\\nPer-class AUROC:\")\n",
    "for i, auc in enumerate(per_class_auc):\n",
    "    ci = per_class_ci[i]\n",
    "    print(f\" Class {i:02d}: AUROC = {auc:.4f}   95% CI = [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "print(f\"\\nMean AUROC: {mean_auc:.4f}\")\n",
    "\n",
    "# === save outputs ===\n",
    "out_dir = \"./eval_results\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "np.save(os.path.join(out_dir, \"test_probs.npy\"), all_probs)\n",
    "np.save(os.path.join(out_dir, \"test_gt.npy\"), all_gt)\n",
    "with open(os.path.join(out_dir, \"per_class_auc.txt\"), \"w\") as f:\n",
    "    f.write(\"class,auc,ci_low,ci_high\\n\")\n",
    "    for i, auc in enumerate(per_class_auc):\n",
    "        ci = per_class_ci[i]\n",
    "        f.write(f\"{i},{auc},{ci[0]},{ci[1]}\\n\")\n",
    "print(\"Saved results to\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a8eb552-091f-4055-a4da-73e7cc950209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in c:\\users\\ansh\\appdata\\roaming\\python\\python311\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ansh\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d24ce566-72ad-42e8-b1a4-9c8b01195cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200/4905] batches processed; avg_loss_so_far=0.2344\n",
      "[400/4905] batches processed; avg_loss_so_far=0.2344\n",
      "[600/4905] batches processed; avg_loss_so_far=0.2344\n",
      "[800/4905] batches processed; avg_loss_so_far=0.2350\n",
      "[1000/4905] batches processed; avg_loss_so_far=0.2353\n",
      "[1200/4905] batches processed; avg_loss_so_far=0.2351\n",
      "[1400/4905] batches processed; avg_loss_so_far=0.2352\n",
      "[1600/4905] batches processed; avg_loss_so_far=0.2355\n",
      "[1800/4905] batches processed; avg_loss_so_far=0.2357\n",
      "[2000/4905] batches processed; avg_loss_so_far=0.2358\n",
      "[2200/4905] batches processed; avg_loss_so_far=0.2360\n",
      "[2400/4905] batches processed; avg_loss_so_far=0.2358\n",
      "[2600/4905] batches processed; avg_loss_so_far=0.2356\n",
      "[2800/4905] batches processed; avg_loss_so_far=0.2355\n",
      "[3000/4905] batches processed; avg_loss_so_far=0.2357\n",
      "[3200/4905] batches processed; avg_loss_so_far=0.2358\n",
      "[3400/4905] batches processed; avg_loss_so_far=0.2359\n",
      "[3600/4905] batches processed; avg_loss_so_far=0.2360\n",
      "[3800/4905] batches processed; avg_loss_so_far=0.2360\n",
      "[4000/4905] batches processed; avg_loss_so_far=0.2360\n",
      "[4200/4905] batches processed; avg_loss_so_far=0.2360\n",
      "[4400/4905] batches processed; avg_loss_so_far=0.2358\n",
      "[4600/4905] batches processed; avg_loss_so_far=0.2357\n",
      "[4800/4905] batches processed; avg_loss_so_far=0.2356\n",
      "[4905/4905] batches processed; avg_loss_so_far=0.9421\n",
      "Full-head epoch done, avg loss: 0.23554874356292393\n",
      "Validation mean AUROC after full head epoch: 0.5093282789775632\n",
      "Saved full-head checkpoint to model_head_epoch1.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# Cell C: Full head-only epoch training (run after Cell A/B confirm)( only to RUN AFTER SUCCESSFUL CELL B)\n",
    "from tqdm import tqdm\n",
    "\n",
    "# assume train_loader, val_loader, model, criterion, optimizer, device already defined\n",
    "log_every = 200\n",
    "model.train()\n",
    "running_loss = 0.0\n",
    "total_batches = len(train_loader)\n",
    "for i, (imgs, targets) in enumerate(train_loader, 1):\n",
    "    imgs = imgs.to(device); targets = targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(imgs)\n",
    "    loss = criterion(logits, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item() * imgs.size(0)\n",
    "    if i % log_every == 0 or i == total_batches:\n",
    "        avg_loss = running_loss / (i * imgs.size(0))\n",
    "        print(f\"[{i}/{total_batches}] batches processed; avg_loss_so_far={avg_loss:.4f}\")\n",
    "\n",
    "epoch_loss = running_loss / len(train_loader.dataset)\n",
    "print(\"Full-head epoch done, avg loss:\", epoch_loss)\n",
    "\n",
    "# run validation (same as earlier) and compute mean_auc\n",
    "model.eval()\n",
    "all_logits, all_gt = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, targets in val_loader:\n",
    "        imgs = imgs.to(device); targets = targets.to(device)\n",
    "        logits = model(imgs)\n",
    "        all_logits.append(logits.cpu()); all_gt.append(targets.cpu())\n",
    "all_logits = torch.cat(all_logits, dim=0)\n",
    "all_gt = torch.cat(all_gt, dim=0)\n",
    "probs = torch.sigmoid(all_logits).numpy()\n",
    "gt_np = all_gt.numpy()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "aucs = []\n",
    "for i in range(N_CLASSES):\n",
    "    if len(np.unique(gt_np[:,i])) < 2:\n",
    "        aucs.append(float('nan'))\n",
    "    else:\n",
    "        try:\n",
    "            aucs.append(float(roc_auc_score(gt_np[:,i], probs[:,i])))\n",
    "        except Exception:\n",
    "            aucs.append(float('nan'))\n",
    "mean_auc = np.nanmean(aucs)\n",
    "print(\"Validation mean AUROC after full head epoch:\", mean_auc)\n",
    "\n",
    "# save checkpoint\n",
    "torch.save({'epoch': 1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'val_mean_auc': float(mean_auc)}, \"model_head_epoch1.pth.tar\")\n",
    "print(\"Saved full-head checkpoint to model_head_epoch1.pth.tar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "537ea14f-b9e2-4e41-a0d7-5804dc6abb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: .\\model_head_epoch1.pth.tar\n",
      "Using test list: .\\ChestX-ray14\\labels\\test_list_split.txt\n",
      "Test dataset size: 22433\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from: .\\model_head_epoch1.pth.tar\n",
      "Predictions shape: (22433, 14) GT shape: (22433, 14)\n",
      "\n",
      "Per-class AUROC:\n",
      " Class 00: AUROC = 0.5372\n",
      " Class 01: AUROC = 0.4435\n",
      " Class 02: AUROC = 0.5371\n",
      " Class 03: AUROC = 0.5169\n",
      " Class 04: AUROC = 0.5075\n",
      " Class 05: AUROC = 0.4988\n",
      " Class 06: AUROC = 0.4737\n",
      " Class 07: AUROC = 0.4769\n",
      " Class 08: AUROC = 0.5046\n",
      " Class 09: AUROC = 0.5937\n",
      " Class 10: AUROC = 0.4663\n",
      " Class 11: AUROC = 0.4502\n",
      " Class 12: AUROC = 0.4484\n",
      " Class 13: AUROC = 0.3255\n",
      "\n",
      "Mean AUROC: 0.4843\n",
      "Saved evaluation outputs to ./eval_results_head/\n"
     ]
    }
   ],
   "source": [
    "# === Cell D: Evaluate full-head checkpoint ===\n",
    "import os, torch, torchvision, numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# === Paths ===\n",
    "DATA_DIR = os.path.join(\".\", \"ChestX-ray14\", \"images\")\n",
    "TEST_IMAGE_LIST = os.path.join(\".\", \"ChestX-ray14\", \"labels\", \"test_list_split.txt\")\n",
    "CKPT_PATH = os.path.join(\".\", \"model_head_epoch1.pth.tar\")\n",
    "N_CLASSES = 14\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "\n",
    "print(\"Using checkpoint:\", CKPT_PATH)\n",
    "print(\"Using test list:\", TEST_IMAGE_LIST)\n",
    "if not os.path.exists(CKPT_PATH):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found at {CKPT_PATH}\")\n",
    "if not os.path.exists(TEST_IMAGE_LIST):\n",
    "    raise FileNotFoundError(f\"Test list not found at {TEST_IMAGE_LIST}\")\n",
    "\n",
    "# === Dataset & dataloader ===\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                                     std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "try:\n",
    "    from read_data import ChestXrayDataSet\n",
    "except Exception as e:\n",
    "    raise ImportError(f\"Failed to import ChestXrayDataSet from read_data: {e}\")\n",
    "\n",
    "test_dataset = ChestXrayDataSet(data_dir=DATA_DIR,s\n",
    "                                image_list_file=TEST_IMAGE_LIST,\n",
    "                                transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False,\n",
    "                         num_workers=NUM_WORKERS,\n",
    "                         pin_memory=PIN_MEMORY)\n",
    "print(\"Test dataset size:\", len(test_dataset))\n",
    "\n",
    "# === Model definition (reuse DenseNet121) ===\n",
    "import torch.nn as nn\n",
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super().__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Linear(num_ftrs, out_size)\n",
    "    def forward(self, x):\n",
    "        return self.densenet121(x)\n",
    "\n",
    "# === Load checkpoint ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "state_dict = ckpt.get('state_dict', ckpt.get('model_state_dict', ckpt))\n",
    "\n",
    "# Strip 'module.' prefix if needed\n",
    "new_state = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    nk = k[len('module.'):] if k.startswith('module.') else k\n",
    "    new_state[nk] = v\n",
    "\n",
    "model = DenseNet121(N_CLASSES).to(device)\n",
    "model.load_state_dict(new_state, strict=False)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully from:\", CKPT_PATH)\n",
    "\n",
    "# === Inference ===\n",
    "all_probs, all_gt = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, targets in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        if isinstance(targets, torch.Tensor):\n",
    "            all_gt.append(targets.cpu().numpy())\n",
    "        else:\n",
    "            all_gt.append(np.asarray(targets))\n",
    "\n",
    "all_probs = np.vstack(all_probs)\n",
    "all_gt = np.vstack(all_gt)\n",
    "print(\"Predictions shape:\", all_probs.shape, \"GT shape:\", all_gt.shape)\n",
    "\n",
    "# === Compute AUROC ===\n",
    "per_class_auc = []\n",
    "for i in range(N_CLASSES):\n",
    "    y_true = all_gt[:, i]\n",
    "    y_score = all_probs[:, i]\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        per_class_auc.append(float('nan'))\n",
    "    else:\n",
    "        per_class_auc.append(float(roc_auc_score(y_true, y_score)))\n",
    "\n",
    "mean_auc = np.nanmean(per_class_auc)\n",
    "print(\"\\nPer-class AUROC:\")\n",
    "for i, auc in enumerate(per_class_auc):\n",
    "    print(f\" Class {i:02d}: AUROC = {auc:.4f}\")\n",
    "print(f\"\\nMean AUROC: {mean_auc:.4f}\")\n",
    "\n",
    "# === Save results ===\n",
    "os.makedirs(\"./eval_results_head\", exist_ok=True)\n",
    "np.save(\"./eval_results_head/test_probs.npy\", all_probs)\n",
    "np.save(\"./eval_results_head/test_gt.npy\", all_gt)\n",
    "with open(\"./eval_results_head/per_class_auc.txt\", \"w\") as f:\n",
    "    f.write(\"class,auc\\n\")\n",
    "    for i, auc in enumerate(per_class_auc):\n",
    "        f.write(f\"{i},{auc}\\n\")\n",
    "print(\"Saved evaluation outputs to ./eval_results_head/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32abdee7-4a3e-45e4-9090-acc129960841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives per class: [2420.0, 582.0, 2754.0, 3938.0, 1133.0, 1335.0, 242.0, 1089.0, 957.0, 413.0, 509.0, 362.0, 734.0, 42.0]\n",
      "Negatives per class: [20013.0, 21851.0, 19679.0, 18495.0, 21300.0, 21098.0, 22191.0, 21344.0, 21476.0, 22020.0, 21924.0, 22071.0, 21699.0, 22391.0]\n",
      "Suggested pos_weight tensor (for BCEWithLogitsLoss): tensor([  8.2698,  37.5447,   7.1456,   4.6965,  18.7996,  15.8037,  91.6983,\n",
      "         19.5996,  22.4410,  53.3172,  43.0727,  60.9696,  29.5627, 533.1190],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Run this to get label counts and pos_weight suggestion\n",
    "import numpy as np, os, torch\n",
    "gt = np.load(\"./eval_results/test_gt.npy\") if os.path.exists(\"./eval_results/test_gt.npy\") else np.load(\"./eval_results_head/test_gt.npy\")\n",
    "pos = gt.sum(axis=0)\n",
    "neg = gt.shape[0] - pos\n",
    "print(\"Positives per class:\", pos.tolist())\n",
    "print(\"Negatives per class:\", neg.tolist())\n",
    "# pos_weight = neg/pos (torch expects float tensor)\n",
    "pos_weight = torch.tensor((neg/(pos+1e-6)).astype(float))\n",
    "print(\"Suggested pos_weight tensor (for BCEWithLogitsLoss):\", pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4fa5eb-cec7-437f-93fd-04df7cf0e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66ebe8ae-dc92-4a5e-a59f-07771ca6cb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val sizes: 78468 11219\n",
      "Computed pos_weight from train dataset (first classes): [ 8.8134067  39.24        7.47295109  4.63949978 18.67602808 16.93554286]\n",
      "Clipped pos_weight (max=50.0): [ 8.813407  39.24       7.472951   4.6394997 18.676027  16.935543\n",
      " 50.        20.178947  23.04781   45.43077   42.617565  50.\n",
      " 33.43089   50.       ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded initial weights from: model_head_epoch1.pth.tar\n",
      "Epoch 1/3 [200/4905] avg_loss_so_far=1.3006\n",
      "Epoch 1/3 [400/4905] avg_loss_so_far=1.2522\n",
      "Epoch 1/3 [600/4905] avg_loss_so_far=1.2256\n",
      "Epoch 1/3 [800/4905] avg_loss_so_far=1.2182\n",
      "Epoch 1/3 [1000/4905] avg_loss_so_far=1.2082\n",
      "Epoch 1/3 [1200/4905] avg_loss_so_far=1.1953\n",
      "Epoch 1/3 [1400/4905] avg_loss_so_far=1.1914\n",
      "Epoch 1/3 [1600/4905] avg_loss_so_far=1.1917\n",
      "Epoch 1/3 [1800/4905] avg_loss_so_far=1.1890\n",
      "Epoch 1/3 [2000/4905] avg_loss_so_far=1.1840\n",
      "Epoch 1/3 [2200/4905] avg_loss_so_far=1.1823\n",
      "Epoch 1/3 [2400/4905] avg_loss_so_far=1.1824\n",
      "Epoch 1/3 [2600/4905] avg_loss_so_far=1.1820\n",
      "Epoch 1/3 [2800/4905] avg_loss_so_far=1.1760\n",
      "Epoch 1/3 [3000/4905] avg_loss_so_far=1.1744\n",
      "Epoch 1/3 [3200/4905] avg_loss_so_far=1.1748\n",
      "Epoch 1/3 [3400/4905] avg_loss_so_far=1.1728\n",
      "Epoch 1/3 [3600/4905] avg_loss_so_far=1.1710\n",
      "Epoch 1/3 [3800/4905] avg_loss_so_far=1.1685\n",
      "Epoch 1/3 [4000/4905] avg_loss_so_far=1.1676\n",
      "Epoch 1/3 [4200/4905] avg_loss_so_far=1.1669\n",
      "Epoch 1/3 [4400/4905] avg_loss_so_far=1.1645\n",
      "Epoch 1/3 [4600/4905] avg_loss_so_far=1.1622\n",
      "Epoch 1/3 [4800/4905] avg_loss_so_far=1.1621\n",
      "Epoch 1 train avg loss: 1.1609\n",
      "Epoch 1 VAL mean AUROC: 0.7351\n",
      "Saved new best head checkpoint: model_head_best.pth.tar val_mean_auc: 0.7351283140529092\n",
      "Epoch 2/3 [200/4905] avg_loss_so_far=1.1374\n",
      "Epoch 2/3 [400/4905] avg_loss_so_far=1.1349\n",
      "Epoch 2/3 [600/4905] avg_loss_so_far=1.1173\n",
      "Epoch 2/3 [800/4905] avg_loss_so_far=1.1238\n",
      "Epoch 2/3 [1000/4905] avg_loss_so_far=1.1335\n",
      "Epoch 2/3 [1200/4905] avg_loss_so_far=1.1353\n",
      "Epoch 2/3 [1400/4905] avg_loss_so_far=1.1396\n",
      "Epoch 2/3 [1600/4905] avg_loss_so_far=1.1344\n",
      "Epoch 2/3 [1800/4905] avg_loss_so_far=1.1362\n",
      "Epoch 2/3 [2000/4905] avg_loss_so_far=1.1369\n",
      "Epoch 2/3 [2200/4905] avg_loss_so_far=1.1363\n",
      "Epoch 2/3 [2400/4905] avg_loss_so_far=1.1379\n",
      "Epoch 2/3 [2600/4905] avg_loss_so_far=1.1370\n",
      "Epoch 2/3 [2800/4905] avg_loss_so_far=1.1366\n",
      "Epoch 2/3 [3000/4905] avg_loss_so_far=1.1336\n",
      "Epoch 2/3 [3200/4905] avg_loss_so_far=1.1358\n",
      "Epoch 2/3 [3400/4905] avg_loss_so_far=1.1356\n",
      "Epoch 2/3 [3600/4905] avg_loss_so_far=1.1341\n",
      "Epoch 2/3 [3800/4905] avg_loss_so_far=1.1348\n",
      "Epoch 2/3 [4000/4905] avg_loss_so_far=1.1343\n",
      "Epoch 2/3 [4200/4905] avg_loss_so_far=1.1364\n",
      "Epoch 2/3 [4400/4905] avg_loss_so_far=1.1377\n",
      "Epoch 2/3 [4600/4905] avg_loss_so_far=1.1388\n",
      "Epoch 2/3 [4800/4905] avg_loss_so_far=1.1383\n",
      "Epoch 2 train avg loss: 1.1377\n",
      "Epoch 2 VAL mean AUROC: 0.7422\n",
      "Saved new best head checkpoint: model_head_best.pth.tar val_mean_auc: 0.742199649325828\n",
      "Epoch 3/3 [200/4905] avg_loss_so_far=1.1581\n",
      "Epoch 3/3 [400/4905] avg_loss_so_far=1.1265\n",
      "Epoch 3/3 [600/4905] avg_loss_so_far=1.1264\n",
      "Epoch 3/3 [800/4905] avg_loss_so_far=1.1323\n",
      "Epoch 3/3 [1000/4905] avg_loss_so_far=1.1292\n",
      "Epoch 3/3 [1200/4905] avg_loss_so_far=1.1273\n",
      "Epoch 3/3 [1400/4905] avg_loss_so_far=1.1295\n",
      "Epoch 3/3 [1600/4905] avg_loss_so_far=1.1276\n",
      "Epoch 3/3 [1800/4905] avg_loss_so_far=1.1279\n",
      "Epoch 3/3 [2000/4905] avg_loss_so_far=1.1322\n",
      "Epoch 3/3 [2200/4905] avg_loss_so_far=1.1328\n",
      "Epoch 3/3 [2400/4905] avg_loss_so_far=1.1339\n",
      "Epoch 3/3 [2600/4905] avg_loss_so_far=1.1337\n",
      "Epoch 3/3 [2800/4905] avg_loss_so_far=1.1337\n",
      "Epoch 3/3 [3000/4905] avg_loss_so_far=1.1330\n",
      "Epoch 3/3 [3200/4905] avg_loss_so_far=1.1363\n",
      "Epoch 3/3 [3400/4905] avg_loss_so_far=1.1349\n",
      "Epoch 3/3 [3600/4905] avg_loss_so_far=1.1341\n",
      "Epoch 3/3 [3800/4905] avg_loss_so_far=1.1345\n",
      "Epoch 3/3 [4000/4905] avg_loss_so_far=1.1339\n",
      "Epoch 3/3 [4200/4905] avg_loss_so_far=1.1346\n",
      "Epoch 3/3 [4400/4905] avg_loss_so_far=1.1345\n",
      "Epoch 3/3 [4600/4905] avg_loss_so_far=1.1345\n",
      "Epoch 3/3 [4800/4905] avg_loss_so_far=1.1344\n",
      "Epoch 3 train avg loss: 1.1348\n",
      "Epoch 3 VAL mean AUROC: 0.7426\n",
      "Saved new best head checkpoint: model_head_best.pth.tar val_mean_auc: 0.7425821287963229\n",
      "Head training complete. Best val mean AUROC: 0.7425821287963229\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 2: head training (3 epochs) with clipped pos_weight =====\n",
    "import os, torch, numpy as np, random\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import OrderedDict\n",
    "\n",
    "# -------- config ----------\n",
    "DATA_DIR = os.path.join(\".\", \"ChestX-ray14\", \"images\")\n",
    "labels_dir = os.path.join(\".\", \"ChestX-ray14\", \"labels\")\n",
    "TRAIN_LIST = os.path.join(labels_dir, \"train_list.txt\")\n",
    "VAL_LIST   = os.path.join(labels_dir, \"val_list.txt\")\n",
    "N_CLASSES = 14\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 3\n",
    "LOG_EVERY = 200\n",
    "CKPT_BEST = \"model_head_best.pth.tar\"\n",
    "MAX_POS_WEIGHT = 50.0   # clip pos_weight to this value to avoid instability\n",
    "\n",
    "# -------- transforms ----------\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "val_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# -------- dataset & dataloader (your dataset class) ----------\n",
    "from read_data import ChestXrayDataSet\n",
    "train_ds = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=TRAIN_LIST, transform=train_transform)\n",
    "val_ds   = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=VAL_LIST, transform=val_transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(\"Train/Val sizes:\", len(train_ds), len(val_ds))\n",
    "\n",
    "# -------- compute pos_weight from training labels (preferred) ----------\n",
    "# This block tries to extract labels from dataset items. If your ChestXrayDataSet returns (img, labels) as numpy/torch, it will work.\n",
    "pos_counts = np.zeros(N_CLASSES, dtype=float)\n",
    "total_samples = 0\n",
    "for i in range(len(train_ds)):\n",
    "    try:\n",
    "        item = train_ds[i]\n",
    "        # expect (img, labels) where labels is array/tensor of shape (N_CLASSES,)\n",
    "        labels = item[1]\n",
    "        if isinstance(labels, torch.Tensor):\n",
    "            labels = labels.cpu().numpy()\n",
    "        labels = np.asarray(labels).astype(float)\n",
    "        pos_counts += labels\n",
    "        total_samples += 1\n",
    "    except Exception:\n",
    "        # fallback: stop scanning if dataset indexing is slow; you'll still be okay using precomputed pos_weight\n",
    "        break\n",
    "\n",
    "if total_samples > 0 and pos_counts.sum() > 0:\n",
    "    neg_counts = total_samples - pos_counts\n",
    "    pos_weight = (neg_counts / (pos_counts + 1e-8)).astype(float)\n",
    "    print(\"Computed pos_weight from train dataset (first classes):\", pos_weight[:6])\n",
    "else:\n",
    "    # fallback: use your provided numbers (from earlier). Replace the list below if you have different numbers.\n",
    "    pos = np.array([2420., 582., 2754., 3938., 1133., 1335., 242., 1089., 957., 413., 509., 362., 734., 42.])\n",
    "    neg = np.array([20013.,21851.,19679.,18495.,21300.,21098.,22191.,21344.,21476.,22020.,21924.,22071.,21699.,22391.])\n",
    "    pos_weight = (neg/(pos+1e-6)).astype(float)\n",
    "    print(\"Using fallback pos_weight (from provided counts):\", pos_weight)\n",
    "\n",
    "# clip pos_weight to a max value to stabilize training\n",
    "pos_weight_clipped = np.minimum(pos_weight, MAX_POS_WEIGHT).astype(np.float32)\n",
    "print(\"Clipped pos_weight (max={}):\".format(MAX_POS_WEIGHT), pos_weight_clipped)\n",
    "\n",
    "# convert to torch tensor on device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pos_weight_tensor = torch.tensor(pos_weight_clipped, dtype=torch.float32).to(device)\n",
    "\n",
    "# -------- model setup ----------\n",
    "import torch.nn as nn\n",
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super().__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Linear(num_ftrs, out_size)\n",
    "    def forward(self, x):\n",
    "        return self.densenet121(x)\n",
    "\n",
    "model = DenseNet121(N_CLASSES).to(device)\n",
    "\n",
    "# if you want to initialize from previous checkpoint (optional)\n",
    "for cand in [\"model_head_epoch1.pth.tar\", \"model.pth.tar\"]:\n",
    "    if os.path.exists(cand):\n",
    "        ck = torch.load(cand, map_location=device)\n",
    "        sd = ck.get(\"state_dict\", ck)\n",
    "        new_sd = OrderedDict()\n",
    "        for k,v in sd.items():\n",
    "            nk = k[len(\"module.\"):] if k.startswith(\"module.\") else k\n",
    "            new_sd[nk] = v\n",
    "        model.load_state_dict(new_sd, strict=False)\n",
    "        print(\"Loaded initial weights from:\", cand)\n",
    "        break\n",
    "\n",
    "# freeze backbone, train head only\n",
    "for name, p in model.named_parameters():\n",
    "    if \"densenet121.classifier\" not in name:\n",
    "        p.requires_grad = False\n",
    "\n",
    "# use BCEWithLogitsLoss with pos_weight\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "# -------- training loop with validation & best-checkpoint saving ----------\n",
    "best_val = -1.0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    total = 0\n",
    "    for i, (imgs, targets) in enumerate(train_loader, 1):\n",
    "        imgs = imgs.to(device); targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item() * imgs.size(0)\n",
    "        total += imgs.size(0)\n",
    "        if i % LOG_EVERY == 0:\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS} [{i}/{len(train_loader)}] avg_loss_so_far={running/total:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} train avg loss: {running/total:.4f}\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    all_probs = []; all_gt=[]\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "            all_probs.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            all_gt.append(targets.cpu().numpy() if isinstance(targets, torch.Tensor) else np.asarray(targets))\n",
    "    all_probs = np.vstack(all_probs); all_gt = np.vstack(all_gt)\n",
    "    mean_auc = np.nanmean([roc_auc_score(all_gt[:,i], all_probs[:,i]) if len(np.unique(all_gt[:,i]))>1 else np.nan for i in range(N_CLASSES)])\n",
    "    print(f\"Epoch {epoch+1} VAL mean AUROC: {mean_auc:.4f}\")\n",
    "    if mean_auc > best_val:\n",
    "        best_val = mean_auc\n",
    "        torch.save({'epoch': epoch+1, 'state_dict': model.state_dict(), 'val_mean_auc': float(mean_auc)}, CKPT_BEST)\n",
    "        print(\"Saved new best head checkpoint:\", CKPT_BEST, \"val_mean_auc:\", best_val)\n",
    "\n",
    "print(\"Head training complete. Best val mean AUROC:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "398f89f0-6b10-49e9-9a2d-e2bb138f9355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pos_weight directly from label file (fast)...\n",
      "Found 78468 samples.\n",
      "Raw pos_weight (first 6): [1.13463503e-01 8.81340670e+00 7.84680000e+12 7.84680000e+12\n",
      " 7.84680000e+12 7.84680000e+12]\n",
      "Clipped pos_weight: [ 0.11346351  8.813407   50.         50.         50.         50.\n",
      " 50.         50.         50.         50.         50.         50.\n",
      " 50.         50.        ]\n",
      "Train/Val sizes: 78468 11219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded initial weights from: model_head_epoch1.pth.tar\n",
      "Epoch 1/3 [200/4905] avg_loss_so_far=1.6469\n",
      "Epoch 1/3 [400/4905] avg_loss_so_far=1.5675\n",
      "Epoch 1/3 [600/4905] avg_loss_so_far=1.5489\n",
      "Epoch 1/3 [800/4905] avg_loss_so_far=1.5333\n",
      "Epoch 1/3 [1000/4905] avg_loss_so_far=1.5144\n",
      "Epoch 1/3 [1200/4905] avg_loss_so_far=1.5153\n",
      "Epoch 1/3 [1400/4905] avg_loss_so_far=1.5070\n",
      "Epoch 1/3 [1600/4905] avg_loss_so_far=1.4975\n",
      "Epoch 1/3 [1800/4905] avg_loss_so_far=1.4979\n",
      "Epoch 1/3 [2000/4905] avg_loss_so_far=1.4902\n",
      "Epoch 1/3 [2200/4905] avg_loss_so_far=1.4876\n",
      "Epoch 1/3 [2400/4905] avg_loss_so_far=1.4839\n",
      "Epoch 1/3 [2600/4905] avg_loss_so_far=1.4824\n",
      "Epoch 1/3 [2800/4905] avg_loss_so_far=1.4805\n",
      "Epoch 1/3 [3000/4905] avg_loss_so_far=1.4793\n",
      "Epoch 1/3 [3200/4905] avg_loss_so_far=1.4746\n",
      "Epoch 1/3 [3400/4905] avg_loss_so_far=1.4752\n",
      "Epoch 1/3 [3600/4905] avg_loss_so_far=1.4724\n",
      "Epoch 1/3 [3800/4905] avg_loss_so_far=1.4730\n",
      "Epoch 1/3 [4000/4905] avg_loss_so_far=1.4722\n",
      "Epoch 1/3 [4200/4905] avg_loss_so_far=1.4707\n",
      "Epoch 1/3 [4400/4905] avg_loss_so_far=1.4696\n",
      "Epoch 1/3 [4600/4905] avg_loss_so_far=1.4674\n",
      "Epoch 1/3 [4800/4905] avg_loss_so_far=1.4666\n",
      "Epoch 1 train avg loss: 1.4656\n",
      "Epoch 1 VAL mean AUROC: 0.7338\n",
      "Saved new best head checkpoint: model_head_best.pth.tar val_mean_auc: 0.7337995805581675\n",
      "Epoch 2/3 [200/4905] avg_loss_so_far=1.4244\n",
      "Epoch 2/3 [400/4905] avg_loss_so_far=1.4372\n",
      "Epoch 2/3 [600/4905] avg_loss_so_far=1.4393\n",
      "Epoch 2/3 [800/4905] avg_loss_so_far=1.4431\n",
      "Epoch 2/3 [1000/4905] avg_loss_so_far=1.4396\n",
      "Epoch 2/3 [1200/4905] avg_loss_so_far=1.4417\n",
      "Epoch 2/3 [1400/4905] avg_loss_so_far=1.4469\n",
      "Epoch 2/3 [1600/4905] avg_loss_so_far=1.4484\n",
      "Epoch 2/3 [1800/4905] avg_loss_so_far=1.4508\n",
      "Epoch 2/3 [2000/4905] avg_loss_so_far=1.4497\n",
      "Epoch 2/3 [2200/4905] avg_loss_so_far=1.4512\n",
      "Epoch 2/3 [2400/4905] avg_loss_so_far=1.4478\n",
      "Epoch 2/3 [2600/4905] avg_loss_so_far=1.4468\n",
      "Epoch 2/3 [2800/4905] avg_loss_so_far=1.4470\n",
      "Epoch 2/3 [3000/4905] avg_loss_so_far=1.4454\n",
      "Epoch 2/3 [3200/4905] avg_loss_so_far=1.4467\n",
      "Epoch 2/3 [3400/4905] avg_loss_so_far=1.4443\n",
      "Epoch 2/3 [3600/4905] avg_loss_so_far=1.4417\n",
      "Epoch 2/3 [3800/4905] avg_loss_so_far=1.4409\n",
      "Epoch 2/3 [4000/4905] avg_loss_so_far=1.4422\n",
      "Epoch 2/3 [4200/4905] avg_loss_so_far=1.4425\n",
      "Epoch 2/3 [4400/4905] avg_loss_so_far=1.4416\n",
      "Epoch 2/3 [4600/4905] avg_loss_so_far=1.4410\n",
      "Epoch 2/3 [4800/4905] avg_loss_so_far=1.4423\n",
      "Epoch 2 train avg loss: 1.4430\n",
      "Epoch 2 VAL mean AUROC: 0.7414\n",
      "Saved new best head checkpoint: model_head_best.pth.tar val_mean_auc: 0.741429092829472\n",
      "Epoch 3/3 [200/4905] avg_loss_so_far=1.4821\n",
      "Epoch 3/3 [400/4905] avg_loss_so_far=1.4695\n",
      "Epoch 3/3 [600/4905] avg_loss_so_far=1.4653\n",
      "Epoch 3/3 [800/4905] avg_loss_so_far=1.4640\n",
      "Epoch 3/3 [1000/4905] avg_loss_so_far=1.4553\n",
      "Epoch 3/3 [1200/4905] avg_loss_so_far=1.4514\n",
      "Epoch 3/3 [1400/4905] avg_loss_so_far=1.4485\n",
      "Epoch 3/3 [1600/4905] avg_loss_so_far=1.4495\n",
      "Epoch 3/3 [1800/4905] avg_loss_so_far=1.4486\n",
      "Epoch 3/3 [2000/4905] avg_loss_so_far=1.4415\n",
      "Epoch 3/3 [2200/4905] avg_loss_so_far=1.4436\n",
      "Epoch 3/3 [2400/4905] avg_loss_so_far=1.4440\n",
      "Epoch 3/3 [2600/4905] avg_loss_so_far=1.4470\n",
      "Epoch 3/3 [2800/4905] avg_loss_so_far=1.4433\n",
      "Epoch 3/3 [3000/4905] avg_loss_so_far=1.4413\n",
      "Epoch 3/3 [3200/4905] avg_loss_so_far=1.4416\n",
      "Epoch 3/3 [3400/4905] avg_loss_so_far=1.4416\n",
      "Epoch 3/3 [3600/4905] avg_loss_so_far=1.4400\n",
      "Epoch 3/3 [3800/4905] avg_loss_so_far=1.4381\n",
      "Epoch 3/3 [4000/4905] avg_loss_so_far=1.4369\n",
      "Epoch 3/3 [4200/4905] avg_loss_so_far=1.4362\n",
      "Epoch 3/3 [4400/4905] avg_loss_so_far=1.4363\n",
      "Epoch 3/3 [4600/4905] avg_loss_so_far=1.4354\n",
      "Epoch 3/3 [4800/4905] avg_loss_so_far=1.4364\n",
      "Epoch 3 train avg loss: 1.4373\n",
      "Epoch 3 VAL mean AUROC: 0.7425\n",
      "Saved new best head checkpoint: model_head_best.pth.tar val_mean_auc: 0.7425141479377696\n",
      "✅ FAST head training complete. Best val mean AUROC: 0.7425141479377696\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 2 (FAST VERSION): head training (3 epochs) with clipped pos_weight =====\n",
    "import os, torch, numpy as np, random\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import OrderedDict\n",
    "\n",
    "# -------- config ----------\n",
    "DATA_DIR = os.path.join(\".\", \"ChestX-ray14\", \"images\")\n",
    "labels_dir = os.path.join(\".\", \"ChestX-ray14\", \"labels\")\n",
    "TRAIN_LIST = os.path.join(labels_dir, \"train_list.txt\")\n",
    "VAL_LIST   = os.path.join(labels_dir, \"val_list.txt\")\n",
    "N_CLASSES = 14\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "NUM_EPOCHS = 3\n",
    "LOG_EVERY = 200\n",
    "CKPT_BEST = \"model_head_best.pth.tar\"\n",
    "MAX_POS_WEIGHT = 50.0   # clip pos_weight to this value to avoid instability\n",
    "\n",
    "# -------- transforms ----------\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "val_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# -------- fast label-based pos_weight computation ----------\n",
    "# This avoids loading all images — reads label text directly.\n",
    "def compute_pos_weight_from_list(label_list_path, n_classes=14):\n",
    "    pos_counts = np.zeros(n_classes, dtype=float)\n",
    "    total = 0\n",
    "    with open(label_list_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 2: \n",
    "                continue\n",
    "            labels_str = parts[1].split(',')\n",
    "            labels = np.zeros(n_classes, dtype=float)\n",
    "            for lab in labels_str:\n",
    "                try:\n",
    "                    idx = int(lab)\n",
    "                    if 0 <= idx < n_classes:\n",
    "                        labels[idx] = 1.0\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            pos_counts += labels\n",
    "            total += 1\n",
    "    neg_counts = total - pos_counts\n",
    "    pos_weight = neg_counts / (pos_counts + 1e-8)\n",
    "    return pos_weight, total\n",
    "\n",
    "print(\"Computing pos_weight directly from label file (fast)...\")\n",
    "pos_weight, total = compute_pos_weight_from_list(TRAIN_LIST, N_CLASSES)\n",
    "print(f\"Found {int(total)} samples.\")\n",
    "print(\"Raw pos_weight (first 6):\", pos_weight[:6])\n",
    "pos_weight = np.minimum(pos_weight, MAX_POS_WEIGHT).astype(np.float32)\n",
    "print(\"Clipped pos_weight:\", pos_weight)\n",
    "\n",
    "# convert to torch tensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pos_weight_tensor = torch.tensor(pos_weight, dtype=torch.float32).to(device)\n",
    "\n",
    "# -------- dataset & dataloader ----------\n",
    "from read_data import ChestXrayDataSet\n",
    "train_ds = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=TRAIN_LIST, transform=train_transform)\n",
    "val_ds   = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=VAL_LIST, transform=val_transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(\"Train/Val sizes:\", len(train_ds), len(val_ds))\n",
    "\n",
    "# -------- model setup ----------\n",
    "import torch.nn as nn\n",
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super().__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Linear(num_ftrs, out_size)\n",
    "    def forward(self, x):\n",
    "        return self.densenet121(x)\n",
    "\n",
    "model = DenseNet121(N_CLASSES).to(device)\n",
    "\n",
    "# if checkpoint exists, load initial weights\n",
    "for cand in [\"model_head_epoch1.pth.tar\", \"model.pth.tar\"]:\n",
    "    if os.path.exists(cand):\n",
    "        ck = torch.load(cand, map_location=device)\n",
    "        sd = ck.get(\"state_dict\", ck)\n",
    "        new_sd = OrderedDict((k[len(\"module.\"):] if k.startswith(\"module.\") else k, v) for k,v in sd.items())\n",
    "        model.load_state_dict(new_sd, strict=False)\n",
    "        print(\"Loaded initial weights from:\", cand)\n",
    "        break\n",
    "\n",
    "# freeze backbone\n",
    "for name, p in model.named_parameters():\n",
    "    if \"densenet121.classifier\" not in name:\n",
    "        p.requires_grad = False\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "# -------- training loop ----------\n",
    "best_val = -1.0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running = total = 0\n",
    "    for i, (imgs, targets) in enumerate(train_loader, 1):\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item() * imgs.size(0)\n",
    "        total += imgs.size(0)\n",
    "        if i % LOG_EVERY == 0:\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCHS} [{i}/{len(train_loader)}] avg_loss_so_far={running/total:.4f}\")\n",
    "    print(f\"Epoch {epoch+1} train avg loss: {running/total:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_probs, all_gt = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "            all_probs.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            all_gt.append(targets.cpu().numpy())\n",
    "    all_probs, all_gt = np.vstack(all_probs), np.vstack(all_gt)\n",
    "    mean_auc = np.nanmean([roc_auc_score(all_gt[:,i], all_probs[:,i]) if len(np.unique(all_gt[:,i]))>1 else np.nan for i in range(N_CLASSES)])\n",
    "    print(f\"Epoch {epoch+1} VAL mean AUROC: {mean_auc:.4f}\")\n",
    "    if mean_auc > best_val:\n",
    "        best_val = mean_auc\n",
    "        torch.save({'epoch': epoch+1, 'state_dict': model.state_dict(), 'val_mean_auc': float(mean_auc)}, CKPT_BEST)\n",
    "        print(\"Saved new best head checkpoint:\", CKPT_BEST, \"val_mean_auc:\", best_val)\n",
    "\n",
    "print(\"✅ FAST head training complete. Best val mean AUROC:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e682218-ad27-416e-9ba8-5dd8ba2b553d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> no checkpoint found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansh\\AppData\\Local\\Temp\\ipykernel_29008\\3778049239.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# -----------------------------------------------------\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mamp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m autocast\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m         output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# (bs * n_crops, n_classes)\u001b[39;00m\n\u001b[32m     72\u001b[39m         output_mean = output.view(bs, n_crops, -\u001b[32m1\u001b[39m).mean(\u001b[32m1\u001b[39m)   \u001b[38;5;66;03m# (bs, n_classes)\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# accumulate predictions to pred (keep pred on GPU or CPU as you prefer)\u001b[39;00m\n\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# change in code here\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mDenseNet121.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdensenet121\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\densenet.py:213\u001b[39m, in \u001b[36mDenseNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     out = F.relu(features, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    215\u001b[39m     out = F.adaptive_avg_pool2d(out, (\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\densenet.py:122\u001b[39m, in \u001b[36m_DenseBlock.forward\u001b[39m\u001b[34m(self, init_features)\u001b[39m\n\u001b[32m    120\u001b[39m features = [init_features]\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items():\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     new_features = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     features.append(new_features)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(features, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\densenet.py:90\u001b[39m, in \u001b[36m_DenseLayer.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     88\u001b[39m     bottleneck_output = \u001b[38;5;28mself\u001b[39m.bn_function(prev_features)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m new_features = \u001b[38;5;28mself\u001b[39m.conv2(\u001b[38;5;28mself\u001b[39m.relu2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbottleneck_output\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.drop_rate > \u001b[32m0\u001b[39m:\n\u001b[32m     92\u001b[39m     new_features = F.dropout(new_features, p=\u001b[38;5;28mself\u001b[39m.drop_rate, training=\u001b[38;5;28mself\u001b[39m.training)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:2822\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ## we DONT EXECUTE THIS YET , INSTEAD WE EXECUTE THE CELL BELOW IT FIRST --> AVOID THIS CELL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ---- REPLACE current DenseNet121 class with this ---- \n",
    "# class DenseNet121(nn.Module):\n",
    "#     \"\"\"DenseNet121 backbone with a linear multi-label head (no Sigmoid).\"\"\"\n",
    "    \n",
    "#     def __init__(self, out_size):\n",
    "#         super(DenseNet121, self).__init__()\n",
    "#         # Using pretrained weights; newer versions may warn about 'pretrained', but this works\n",
    "#         self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        \n",
    "#         # Get the number of input features to the classifier\n",
    "#         num_ftrs = self.densenet121.classifier.in_features\n",
    "        \n",
    "#         # Replace the classifier with a linear layer that outputs logits\n",
    "#         # (No sigmoid activation here — handle it in the loss function if needed)\n",
    "#         self.densenet121.classifier = nn.Linear(num_ftrs, out_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.densenet121(x)\n",
    "#         return x\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b03999d0-0a9d-4a10-bfdb-69b56fdaec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "train_list.txt and val_list.txt already present.\n",
      "Train/Val sizes: 78468 11219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Ansh\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch done, avg loss: 0.1777671042022862\n",
      "Validation mean AUROC (smoke-test): 0.737075715617487\n",
      "Per-class AUROC (first 6): [0.7167493076385387, 0.7312389941403284, 0.773260474840952, 0.646305061855538, 0.6773198414196715, 0.6412038155016444]\n",
      "Saved checkpoint to model.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# # ---- REPLACE current DenseNet121 class with this ----\n",
    "# class DenseNet121(nn.Module):\n",
    "#     \"\"\"DenseNet121 backbone with a linear multi-label head (no Sigmoid).\"\"\"\n",
    "    \n",
    "#     def __init__(self, out_size):\n",
    "#         super(DenseNet121, self).__init__()\n",
    "#         # Using pretrained weights; newer versions may warn about 'pretrained', but this works\n",
    "#         self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        \n",
    "#         # Get the number of input features to the classifier\n",
    "#         num_ftrs = self.densenet121.classifier.in_features\n",
    "        \n",
    "#         # Replace the classifier with a linear layer that outputs logits\n",
    "#         # (No sigmoid activation here — handle it in the loss function if needed)\n",
    "#         self.densenet121.classifier = nn.Linear(num_ftrs, out_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.densenet121(x)\n",
    "#         return x\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ---------- TRAINING SMOKE-TEST CELL ----------\n",
    "# import os, random, numpy as np, torch\n",
    "# import torch.nn as nn, torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# import torchvision\n",
    "\n",
    "# # ---------- 1) basic env + reproducibility ----------\n",
    "# seed = 42\n",
    "# random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "# if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Device:\", device)\n",
    "\n",
    "# # ---------- 2) ensure train/val lists exist (split test_list.txt if needed) ----------\n",
    "# labels_dir = os.path.join(\".\", \"ChestX-ray14\", \"labels\")\n",
    "# os.makedirs(labels_dir, exist_ok=True)\n",
    "# master_list = os.path.join(labels_dir, \"test_list.txt\")\n",
    "# train_list = os.path.join(labels_dir, \"train_list.txt\")\n",
    "# val_list = os.path.join(labels_dir, \"val_list.txt\")\n",
    "\n",
    "# if not os.path.exists(train_list) or not os.path.exists(val_list):\n",
    "#     if os.path.exists(master_list):\n",
    "#         with open(master_list, 'r') as f:\n",
    "#             lines = [l.strip() for l in f if l.strip()]\n",
    "#         random.shuffle(lines)\n",
    "#         n = len(lines)\n",
    "#         n_train = int(0.8*n); n_val = int(0.1*n)\n",
    "#         train_lines = lines[:n_train]\n",
    "#         val_lines = lines[n_train:n_train+n_val]\n",
    "#         test_lines = lines[n_train+n_val:]\n",
    "#         with open(train_list, 'w') as f: f.write(\"\\n\".join(train_lines))\n",
    "#         with open(val_list, 'w') as f: f.write(\"\\n\".join(val_lines))\n",
    "#         with open(os.path.join(labels_dir, 'test_list_split.txt'), 'w') as f: f.write(\"\\n\".join(test_lines))\n",
    "#         print(\"Created train/val/test split:\", len(train_lines), len(val_lines), len(test_lines))\n",
    "#     else:\n",
    "#         raise FileNotFoundError(f\"No master list found at {master_list}. Place a list or tell me the correct path.\")\n",
    "# else:\n",
    "#     print(\"train_list.txt and val_list.txt already present.\")\n",
    "\n",
    "# # ---------- 3) transforms ----------\n",
    "# train_transform = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.RandomResizedCrop(224),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(),\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "# ])\n",
    "# val_transform = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.Resize(256),\n",
    "#     torchvision.transforms.CenterCrop(224),\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "# ])\n",
    "\n",
    "# # ---------- 4) dataset & dataloader (uses your read_data.ChestXrayDataSet) ----------\n",
    "# try:\n",
    "#     from read_data import ChestXrayDataSet\n",
    "# except Exception as e:\n",
    "#     print(\"ERROR importing ChestXrayDataSet from read_data:\", e)\n",
    "#     raise\n",
    "\n",
    "# DATA_DIR = globals().get('DATA_DIR', os.path.join(\".\", \"ChestX-ray14\", \"images\"))\n",
    "# N_CLASSES = globals().get('N_CLASSES', 14)\n",
    "# CKPT_PATH = globals().get('CKPT_PATH', os.path.join(\".\", \"model.pth.tar\"))\n",
    "\n",
    "# train_dataset = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=train_list, transform=train_transform)\n",
    "# val_dataset   = ChestXrayDataSet(data_dir=DATA_DIR, image_list_file=val_list, transform=val_transform)\n",
    "\n",
    "# BATCH_SIZE = 16   # lower if OOM on your machine\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)\n",
    "# val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "# print(\"Train/Val sizes:\", len(train_dataset), len(val_dataset))\n",
    "\n",
    "# # ---------- 5) model (assumes DenseNet121 class already redefined without Sigmoid) ----------\n",
    "# model = DenseNet121(N_CLASSES).to(device)\n",
    "\n",
    "# # freeze everything except classifier head\n",
    "# for name, p in model.named_parameters():\n",
    "#     if \"densenet121.classifier\" not in name:\n",
    "#         p.requires_grad = False\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "# # ---------- 6) train 1 epoch (smoke test) ----------\n",
    "# model.train()\n",
    "# running_loss = 0.0\n",
    "# for imgs, targets in train_loader:\n",
    "#     imgs = imgs.to(device); targets = targets.to(device)\n",
    "#     optimizer.zero_grad()\n",
    "#     logits = model(imgs)             # logits\n",
    "#     loss = criterion(logits, targets)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     running_loss += loss.item() * imgs.size(0)\n",
    "# print(\"Train epoch done, avg loss:\", running_loss / max(1, len(train_loader.dataset)))\n",
    "\n",
    "# # ---------- 7) quick validation ----------\n",
    "# model.eval()\n",
    "# all_logits, all_gt = [], []\n",
    "# with torch.no_grad():\n",
    "#     for imgs, targets in val_loader:\n",
    "#         imgs = imgs.to(device); targets = targets.to(device)\n",
    "#         logits = model(imgs)\n",
    "#         all_logits.append(logits.cpu()); all_gt.append(targets.cpu())\n",
    "\n",
    "# all_logits = torch.cat(all_logits, dim=0)\n",
    "# all_gt = torch.cat(all_gt, dim=0)\n",
    "# probs = torch.sigmoid(all_logits).numpy()\n",
    "# gt_np = all_gt.numpy()\n",
    "\n",
    "# aucs = []\n",
    "# for i in range(N_CLASSES):\n",
    "#     if len(np.unique(gt_np[:,i])) < 2:\n",
    "#         aucs.append(float('nan'))\n",
    "#     else:\n",
    "#         try:\n",
    "#             aucs.append(float(roc_auc_score(gt_np[:,i], probs[:,i])))\n",
    "#         except Exception:\n",
    "#             aucs.append(float('nan'))\n",
    "\n",
    "# mean_auc = np.nanmean(aucs)\n",
    "# print(\"Validation mean AUROC (smoke-test):\", mean_auc)\n",
    "# print(\"Per-class AUROC (first 6):\", aucs[:6])\n",
    "\n",
    "# # ---------- 8) save checkpoint ----------\n",
    "# state = {'epoch': 1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'val_mean_auc': float(mean_auc)}\n",
    "# torch.save(state, CKPT_PATH)\n",
    "# print(\"Saved checkpoint to\", CKPT_PATH)\n",
    "# # ---------- END ----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa46a9b-f87e-42b6-a0c5-c1b7a34e7e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "CUDA current device index: 0\n",
      "CUDA device name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Memory allocated (bytes): 0\n",
      "Memory reserved (bytes): 0\n",
      "Model not found in this scope or error: name 'model' is not defined\n",
      "device variable in env (if exists): None\n"
     ]
    }
   ],
   "source": [
    "##this is to see that whether the gpu is being used or the cpu is being used-\n",
    "\n",
    "import torch, os\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "        print(\"CUDA current device index:\", torch.cuda.current_device())\n",
    "        print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "        print(\"Memory allocated (bytes):\", torch.cuda.memory_allocated())\n",
    "        print(\"Memory reserved (bytes):\", torch.cuda.memory_reserved())\n",
    "    except Exception as e:\n",
    "        print(\"Error querying CUDA device:\", e)\n",
    "\n",
    "# check where the model lives (if model is defined in this kernel)\n",
    "try:\n",
    "    p = next(model.parameters())\n",
    "    print(\"Sample model parameter device:\", p.device)\n",
    "except Exception as e:\n",
    "    print(\"Model not found in this scope or error:\", e)\n",
    "\n",
    "# quick check: are we on the device variable you printed earlier?\n",
    "try:\n",
    "    print(\"device variable in env (if exists):\", globals().get('device', None))\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e573ddd-3e11-49b3-a815-8d5a265cf32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the below cell is code for creating a backup safely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6531eb9-1b92-4681-ad7e-8beb6cc29238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed CWD to: C:\\Users\\Ansh\\Desktop\\ml project\n",
      "Backup created: C:\\Users\\Ansh\\Desktop\\ml project\\mlproject_backup.ipynb\n"
     ]
    }
   ],
   "source": [
    "# # Backup the notebook file (edit src_name if your file has a different name)\n",
    "# import os, shutil\n",
    "\n",
    "# # set these to match your environment / notebook filename\n",
    "# notebook_dir = r\"C:\\Users\\Ansh\\Desktop\\ml project\"\n",
    "# os.chdir(notebook_dir)\n",
    "# print(\"Changed CWD to:\", os.getcwd())\n",
    "\n",
    "# src_name = \"mlproject.ipynb\"   # change if your file has a different name\n",
    "# if not os.path.exists(src_name):\n",
    "#     print(f\"File not found: {src_name}. Files in dir:\", os.listdir()[:50])\n",
    "# else:\n",
    "#     dst_name = \"mlproject_backup.ipynb\"\n",
    "#     shutil.copy(src_name, dst_name)\n",
    "#     print(\"Backup created:\", os.path.join(os.getcwd(), dst_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5b7a2-f729-46fe-a7ef-042f14fa423e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
